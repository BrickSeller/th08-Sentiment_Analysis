{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung zur Sentimentanalyse - Analyse von Twitter Daten\n",
    "\n",
    "## Aufgabe 1: Theorie\n",
    "\n",
    "### 1.1: Wozu dient das Package TextBlog von Python und wie wird es angewendet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Frage: Wie muss der Text vorbereitet werden, um den TextBlob Classifier anwenden  zu können?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2: Übung Twitter Data mining\n",
    "## In dieser Übung sollen alle Tweets mit dem Tag \"dataScience\" gesammelt werden.\n",
    "Für die folgende Übung benötigen wir zunächst einen API Key von Twitter.\n",
    "Diesen bekommt man unter [apps.twitter.com](https://apps.twitter.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Die Zugangsdaten sollen in einem Python Modul *credentials.py* abgespeichert werden.\n",
    "Dieses könnte folgendermaßen aussehen.\n",
    "```python\n",
    "# Consume:\n",
    "CONSUMER_KEY    = ''\n",
    "CONSUMER_SECRET = ''\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = ''\n",
    "ACCESS_SECRET = ''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Anschließend benötigen wir Imports für die folgenden Packages:\n",
    "* tweepy\n",
    "* pandas\n",
    "* numpy\n",
    "* credentials\n",
    "* TextBlob\n",
    "* re\n",
    "\n",
    "Für die Visualisierung benötigen wir zusätzlich\n",
    "* Ipython.display\n",
    "* matplotlib\n",
    "* seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Nun wollen wir in einem das API Setup und die Authentication mit Hilfe unseres API Keys aufsetzen\n",
    "Hier sollen nun mit Hilfe des credentials Modules die Authemtication und somit de API Verbindung hergestellt werden.<br>\n",
    "Hilfe beim Setup bietet unter anderem dieses [Tutorial](https://www.sitepoint.com/how-to-create-a-twitter-app-and-api-interface-via-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Als nächstes wollen wir ein Objekt erstellen über welches wir Twitter abfragen tätigen können\n",
    "Die Methode um nach einfachen Tags zu suchen lautet folgendermaßen : \n",
    "``` python\n",
    "search(q= \"SearchTag\")\n",
    "```\n",
    "Anschließend sollen die gesammelten Tweets ausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Um die Tweets zu analysieren sollen diese nun in einen DataFrame gespeichert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Um zu sehen wie ein abgespeicherter Tweet aufgebaut ist, lassen wir uns folgende Attribute des 1. Tweets ausgeben:\n",
    "* id\n",
    "* created_at\n",
    "* source\n",
    "* favorite_count\n",
    "* retweet_count\n",
    "* geo\n",
    "* coordinates\n",
    "* entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Nun wollen wir unserem Dataframe die Attribute Länge des Textex, ID, Date, Source, Likes und Retweets hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Wie lang sind die von uns gesammelten Durchchnittstweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Welcher Tweet hat die meisten Likes bekommen? Welcher Tweet wurde am öftesten retweetet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Um die Daten über Zeit zu analysieren, erstellen wir nun eine Serie für die Attribute len, Likes und Retweets mit dem Attribut Date als index.\n",
    "Dies lässt sich  mit Hilfe der pandas Methode \n",
    "```python\n",
    "pd.series(data,index)\n",
    "```\n",
    "realisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Nun plotten wir die Länge der Tweets über Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Nun plotten wir noch Likes vs Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Über welches Tools wurde getwittert? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 Nach der ersten Analyse des Datensatzes, kennen wir nun dessen Aufbau. <br>Nun wird es Zeit für die eigentliche Sentimentanalyse\n",
    "Zuerst müssen die Tweets gecleaned werden.<br>Das bedeutet wir entfernen folgende Sonderzeichen:\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \"<br> Hierbei hilft uns das Pyackage **re**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14 Nachdem wir die Tweets gecleaned haben, können wir die den TextBlob Classifier verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15 Die Ergebnisse sollen in einer zusätzlichen Spalte z.B. \"Sentiment\" im Dataframe dargestellt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.16 Um eine Übersicht über das Ergebnis zu erhalten, wollen wir uns die Sentimente nun in Prozentangaben ausgeben lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
